{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-05T01:29:26.467106Z","iopub.execute_input":"2023-01-05T01:29:26.467498Z","iopub.status.idle":"2023-01-05T01:29:26.484874Z","shell.execute_reply.started":"2023-01-05T01:29:26.467466Z","shell.execute_reply":"2023-01-05T01:29:26.483843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fetching data from Netflix, AMazon Prime, Hulu and Disney+ movie and TV shows datasets","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/netflix-shows/netflix_titles.csv')\ndf=pd.concat([df,pd.read_csv('/kaggle/input/hulu-movies-and-tv-shows/hulu_titles.csv')])\ndf=pd.concat([df,pd.read_csv('/kaggle/input/amazon-prime-movies-and-tv-shows/amazon_prime_titles.csv')])\ndf=pd.concat([df,pd.read_csv('/kaggle/input/disney-movies-and-tv-shows/disney_plus_titles.csv')])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:07.110135Z","iopub.execute_input":"2023-01-05T02:19:07.110542Z","iopub.status.idle":"2023-01-05T02:19:07.288463Z","shell.execute_reply.started":"2023-01-05T02:19:07.110506Z","shell.execute_reply":"2023-01-05T02:19:07.287481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:10.124743Z","iopub.execute_input":"2023-01-05T02:19:10.125253Z","iopub.status.idle":"2023-01-05T02:19:10.133090Z","shell.execute_reply.started":"2023-01-05T02:19:10.125202Z","shell.execute_reply":"2023-01-05T02:19:10.131544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:11.803998Z","iopub.execute_input":"2023-01-05T02:19:11.804440Z","iopub.status.idle":"2023-01-05T02:19:11.832444Z","shell.execute_reply.started":"2023-01-05T02:19:11.804397Z","shell.execute_reply":"2023-01-05T02:19:11.831386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing null valued rows for title and description","metadata":{}},{"cell_type":"code","source":"drop_indices = list(np.where(df['description'].isna())[0])+list(np.where(df['title'].isna())[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:21.819015Z","iopub.execute_input":"2023-01-05T02:19:21.819388Z","iopub.status.idle":"2023-01-05T02:19:21.831806Z","shell.execute_reply.started":"2023-01-05T02:19:21.819356Z","shell.execute_reply":"2023-01-05T02:19:21.830777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_indices","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:23.518004Z","iopub.execute_input":"2023-01-05T02:19:23.518415Z","iopub.status.idle":"2023-01-05T02:19:23.525624Z","shell.execute_reply.started":"2023-01-05T02:19:23.518380Z","shell.execute_reply":"2023-01-05T02:19:23.524335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop(drop_indices)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:42.547314Z","iopub.execute_input":"2023-01-05T02:19:42.547850Z","iopub.status.idle":"2023-01-05T02:19:42.565681Z","shell.execute_reply.started":"2023-01-05T02:19:42.547814Z","shell.execute_reply":"2023-01-05T02:19:42.564594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:44.595557Z","iopub.execute_input":"2023-01-05T02:19:44.595982Z","iopub.status.idle":"2023-01-05T02:19:44.602921Z","shell.execute_reply.started":"2023-01-05T02:19:44.595948Z","shell.execute_reply":"2023-01-05T02:19:44.601631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:19:48.141320Z","iopub.execute_input":"2023-01-05T02:19:48.141716Z","iopub.status.idle":"2023-01-05T02:19:48.168171Z","shell.execute_reply.started":"2023-01-05T02:19:48.141683Z","shell.execute_reply":"2023-01-05T02:19:48.167078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['title','description']].info()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:20:16.562413Z","iopub.execute_input":"2023-01-05T02:20:16.562805Z","iopub.status.idle":"2023-01-05T02:20:16.583091Z","shell.execute_reply.started":"2023-01-05T02:20:16.562772Z","shell.execute_reply":"2023-01-05T02:20:16.581995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['title','description']].to_csv('/kaggle/working/data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:20:51.103046Z","iopub.execute_input":"2023-01-05T02:20:51.103539Z","iopub.status.idle":"2023-01-05T02:20:51.326982Z","shell.execute_reply.started":"2023-01-05T02:20:51.103488Z","shell.execute_reply":"2023-01-05T02:20:51.325657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Length of inputs (description) and outputs (titles) in the dataset","metadata":{}},{"cell_type":"code","source":"max_length_description= max([len(desc) for desc in df['description']])\nmax_length_title= max([len(tit) for tit in df['title']])\navg_length_description= sum([len(desc) for desc in df['description']])/len(df['description'])\navg_length_title= sum([len(tit) for tit in df['title']])/len(df['title'])\nprint(\"Max Length of description in data \",max_length_description)\nprint(\"Max Length of title in data \",max_length_title)\nprint(\"Average Length of description in data \",avg_length_description)\nprint(\"Average Length of title in data \",avg_length_title)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T03:22:02.925757Z","iopub.execute_input":"2023-01-05T03:22:02.926151Z","iopub.status.idle":"2023-01-05T03:22:02.963370Z","shell.execute_reply.started":"2023-01-05T03:22:02.926118Z","shell.execute_reply":"2023-01-05T03:22:02.962367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install datasets transformers rouge-score nltk","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:21:19.588554Z","iopub.execute_input":"2023-01-05T02:21:19.589332Z","iopub.status.idle":"2023-01-05T02:21:30.681618Z","shell.execute_reply.started":"2023-01-05T02:21:19.589294Z","shell.execute_reply":"2023-01-05T02:21:30.680304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:11:08.911281Z","iopub.execute_input":"2023-01-05T02:11:08.911687Z","iopub.status.idle":"2023-01-05T02:11:08.916915Z","shell.execute_reply.started":"2023-01-05T02:11:08.911650Z","shell.execute_reply":"2023-01-05T02:11:08.915717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netflix=load_dataset(\"csv\",data_files='/kaggle/working/data.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:07.345337Z","iopub.execute_input":"2023-01-05T02:22:07.345737Z","iopub.status.idle":"2023-01-05T02:22:07.579534Z","shell.execute_reply.started":"2023-01-05T02:22:07.345703Z","shell.execute_reply":"2023-01-05T02:22:07.578527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netflix","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:08.569123Z","iopub.execute_input":"2023-01-05T02:22:08.569742Z","iopub.status.idle":"2023-01-05T02:22:08.576627Z","shell.execute_reply.started":"2023-01-05T02:22:08.569679Z","shell.execute_reply":"2023-01-05T02:22:08.575645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test = netflix[\"train\"].train_test_split(test_size=100)\nnetflix[\"train\"] = train_test[\"train\"]\nnetflix[\"test\"] = train_test[\"test\"]\n\ntrain_validation = netflix[\"train\"].train_test_split(test_size=2894)\n\nnetflix[\"train\"] = train_validation[\"train\"]\nnetflix[\"validation\"] = train_validation[\"test\"]\n# netflix[\"test\"] = train_test[\"test\"]\n\nnetflix","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:10.109380Z","iopub.execute_input":"2023-01-05T02:22:10.109805Z","iopub.status.idle":"2023-01-05T02:22:10.146421Z","shell.execute_reply.started":"2023-01-05T02:22:10.109773Z","shell.execute_reply":"2023-01-05T02:22:10.145400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_train = len(netflix[\"train\"])\nn_validation = len(netflix[\"validation\"])\nn_test = len(netflix[\"test\"])\nn_total = n_train + n_validation + n_test\n\nprint(f\"- Training set: {n_train*100/n_total:.2f}%\")\nprint(f\"- Validation set: {n_validation*100/n_total:.2f}%\")\nprint(f\"- Test set: {n_test*100/n_total:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:13.890070Z","iopub.execute_input":"2023-01-05T02:22:13.890456Z","iopub.status.idle":"2023-01-05T02:22:13.897516Z","shell.execute_reply.started":"2023-01-05T02:22:13.890422Z","shell.execute_reply":"2023-01-05T02:22:13.896428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Pre-processing","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nimport string\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:16.130035Z","iopub.execute_input":"2023-01-05T02:22:16.130937Z","iopub.status.idle":"2023-01-05T02:22:16.322589Z","shell.execute_reply.started":"2023-01-05T02:22:16.130896Z","shell.execute_reply":"2023-01-05T02:22:16.321469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"t5-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:17.259812Z","iopub.execute_input":"2023-01-05T02:22:17.260812Z","iopub.status.idle":"2023-01-05T02:22:18.850393Z","shell.execute_reply.started":"2023-01-05T02:22:17.260757Z","shell.execute_reply":"2023-01-05T02:22:18.849175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prefix = \"summarize: \"\n\nmax_input_length = 500\nmax_target_length = 150\n\ndef clean(text):\n    sentences = nltk.sent_tokenize(text.strip())\n    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n                                 if len(sent) > 0 and\n                                 sent[-1] in string.punctuation]\n    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n    return text_cleaned\n\ndef preprocess(examples):\n    texts_cleaned = [clean(text) for text in examples[\"description\"]]\n    inputs = [prefix + text for text in texts_cleaned]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # Setting the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"title\"], max_length=max_target_length, \n                           truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:35.506306Z","iopub.execute_input":"2023-01-05T02:22:35.506718Z","iopub.status.idle":"2023-01-05T02:22:35.515999Z","shell.execute_reply.started":"2023-01-05T02:22:35.506684Z","shell.execute_reply":"2023-01-05T02:22:35.514868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"netflix_cleaned = netflix.filter(lambda example: (len(example['description']) <= 500) and (len(example['title']) <= 150))\ntokenized = netflix_cleaned.map(preprocess, batched=True)\ntokenized","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:22:38.264988Z","iopub.execute_input":"2023-01-05T02:22:38.265391Z","iopub.status.idle":"2023-01-05T02:22:44.611675Z","shell.execute_reply.started":"2023-01-05T02:22:38.265355Z","shell.execute_reply":"2023-01-05T02:22:44.610683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized[\"train\"][6]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:23:17.867968Z","iopub.execute_input":"2023-01-05T02:23:17.868347Z","iopub.status.idle":"2023-01-05T02:23:17.877221Z","shell.execute_reply.started":"2023-01-05T02:23:17.868312Z","shell.execute_reply":"2023-01-05T02:23:17.876089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tuning T5 for Netflix title generation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:23:27.912098Z","iopub.execute_input":"2023-01-05T02:23:27.912540Z","iopub.status.idle":"2023-01-05T02:23:27.918999Z","shell.execute_reply.started":"2023-01-05T02:23:27.912503Z","shell.execute_reply":"2023-01-05T02:23:27.916986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r {model_dir}","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:23:30.806951Z","iopub.execute_input":"2023-01-05T02:23:30.807647Z","iopub.status.idle":"2023-01-05T02:23:31.979191Z","shell.execute_reply.started":"2023-01-05T02:23:30.807604Z","shell.execute_reply":"2023-01-05T02:23:31.977817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initiating training arguments","metadata":{}},{"cell_type":"code","source":"batch_size = 8\nmy_model_name = \"t5-base-title-generation\"\nmodel_dir = f\"{my_model_name}\"\nargs = Seq2SeqTrainingArguments(\n    model_dir,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_strategy=\"steps\",\n    save_steps=200,\n    learning_rate=4e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge1\",\n    report_to=\"tensorboard\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:24:11.377297Z","iopub.execute_input":"2023-01-05T02:24:11.377753Z","iopub.status.idle":"2023-01-05T02:24:11.393069Z","shell.execute_reply.started":"2023-01-05T02:24:11.377712Z","shell.execute_reply":"2023-01-05T02:24:11.391050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:24:13.390377Z","iopub.execute_input":"2023-01-05T02:24:13.391175Z","iopub.status.idle":"2023-01-05T02:24:13.397296Z","shell.execute_reply.started":"2023-01-05T02:24:13.391132Z","shell.execute_reply":"2023-01-05T02:24:13.396281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nmetric = load_metric(\"rouge\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    \n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n                      for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n                      for label in decoded_labels]\n    \n    # Compute ROUGE scores\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n                            use_stemmer=True)\n\n    # Extract ROUGE f1 scores\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n    \n    # Add mean generated length to metrics\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n                      for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:24:14.692624Z","iopub.execute_input":"2023-01-05T02:24:14.692997Z","iopub.status.idle":"2023-01-05T02:24:15.113539Z","shell.execute_reply.started":"2023-01-05T02:24:14.692967Z","shell.execute_reply":"2023-01-05T02:24:15.112483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntrainer = Seq2SeqTrainer(\n    model_init=model_init,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n    # max_length=500\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:24:16.753704Z","iopub.execute_input":"2023-01-05T02:24:16.754856Z","iopub.status.idle":"2023-01-05T02:24:29.174187Z","shell.execute_reply.started":"2023-01-05T02:24:16.754808Z","shell.execute_reply":"2023-01-05T02:24:29.172999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reload_ext tensorboard\n# %tensorboard --logdir '{model_dir}'/runs","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:24:29.176754Z","iopub.execute_input":"2023-01-05T02:24:29.177476Z","iopub.status.idle":"2023-01-05T02:24:29.185697Z","shell.execute_reply.started":"2023-01-05T02:24:29.177429Z","shell.execute_reply":"2023-01-05T02:24:29.184487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T02:33:56.585368Z","iopub.execute_input":"2023-01-05T02:33:56.585762Z","iopub.status.idle":"2023-01-05T03:04:11.416950Z","shell.execute_reply.started":"2023-01-05T02:33:56.585726Z","shell.execute_reply":"2023-01-05T03:04:11.415887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T03:04:13.445531Z","iopub.execute_input":"2023-01-05T03:04:13.446461Z","iopub.status.idle":"2023-01-05T03:04:15.821812Z","shell.execute_reply.started":"2023-01-05T03:04:13.446407Z","shell.execute_reply":"2023-01-05T03:04:15.820718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load and generate","metadata":{}},{"cell_type":"code","source":"model_name = \"t5-base-title-generation\"\nmodel_dir = f\"{model_name}\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n\nmax_input_length = 500","metadata":{"execution":{"iopub.status.busy":"2023-01-05T03:04:15.825449Z","iopub.execute_input":"2023-01-05T03:04:15.826122Z","iopub.status.idle":"2023-01-05T03:04:18.761873Z","shell.execute_reply.started":"2023-01-05T03:04:15.826090Z","shell.execute_reply":"2023-01-05T03:04:18.760577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = netflix[\"test\"][1]['description']\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T03:04:18.763269Z","iopub.execute_input":"2023-01-05T03:04:18.764022Z","iopub.status.idle":"2023-01-05T03:04:18.825647Z","shell.execute_reply.started":"2023-01-05T03:04:18.763985Z","shell.execute_reply":"2023-01-05T03:04:18.824514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = netflix[\"test\"][78]['description']\nprint(text)\ninputs = [\"summarize: \" + text]\n\ninputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\noutput = model.generate(**inputs, num_beams=8, do_sample=True, min_length=30, max_length=150)\ndecoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\npredicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\nprint(\"Title: \")\nprint(predicted_title)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T03:15:35.364755Z","iopub.execute_input":"2023-01-05T03:15:35.365136Z","iopub.status.idle":"2023-01-05T03:15:53.814114Z","shell.execute_reply.started":"2023-01-05T03:15:35.365103Z","shell.execute_reply":"2023-01-05T03:15:53.812848Z"},"trusted":true},"execution_count":null,"outputs":[]}]}